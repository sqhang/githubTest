{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_tuner_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sKwLOzKpFGAj"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sqhang/githubTest/blob/main/keras_tuner_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY-JtR51TVwN"
      },
      "source": [
        "# Source and Copyright\n",
        "This Google Colab notebook was adapted from one tutorial from Keras Tuner. The notebook changes the tuning of the original ANN model to a new CNN model and adds several more functionalities.\n",
        "\n",
        "This notebook is just used for internal learning and nonprofit purposes. All copyrights are reserved for TensorFlow authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Introduction to the Keras Tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/keras_tuner\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called *hyperparameter tuning* or *hypertuning*.\n",
        "\n",
        "Hyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types:\n",
        "1. **Model hyperparameters** which influence model selection such as the number and width of hidden layers\n",
        "2. **Algorithm hyperparameters** which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) and the number of nearest neighbors for a k Nearest Neighbors (KNN) classifier\n",
        "\n",
        "In this tutorial, you will use the Keras Tuner to perform hypertuning for an image classification application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Flatten, LeakyReLU, ReLU, Dropout\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g83Lwsy-Aq2_"
      },
      "source": [
        "Install and import the Keras Tuner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpMLpbt9jcO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656223d6-8fe6-4d74-e8d6-b39921337281"
      },
      "source": [
        "!pip install -q -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10kB 25.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 30.2MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n",
            "\u001b[?25h  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_leAIdFKAxAD"
      },
      "source": [
        "import kerastuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReV_UXOgCZvx"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "In this tutorial, you will use the Keras Tuner to find the best hyperparameters for a machine learning model that classifies images of clothing from the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HljH_ENLEdHa"
      },
      "source": [
        "Load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHlHs9Wj_PUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0574b1a2-864a-4b36-e8d5-86a6e951a202"
      },
      "source": [
        "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZo0tHOXKGOF"
      },
      "source": [
        "Scale the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLVhXs3xrUD0"
      },
      "source": [
        "# Normalize pixel values between 0 and 1\n",
        "img_train = img_train.astype('float32') / 255.0\n",
        "img_test = img_test.astype('float32') / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32VU2Z6mKCFN"
      },
      "source": [
        "Reshape the data for CNN training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS3GvAbSpiSO",
        "outputId": "8b624ead-fdc6-4cde-e66e-edb213b00ff5"
      },
      "source": [
        "# Reshape the dataset for the CNN \n",
        "img_train = img_train[..., np.newaxis]\n",
        "img_test = img_test[..., np.newaxis]\n",
        "img_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5YEL2H2Ax3e"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a *hypermodel*.\n",
        "\n",
        "You can define a hypermodel through two approaches:\n",
        "\n",
        "* By using a model builder function\n",
        "* By subclassing the `HyperModel` class of the Keras Tuner API\n",
        "\n",
        "You can also use two pre-defined `HyperModel` classes - [HyperXception](https://keras-team.github.io/keras-tuner/documentation/hypermodels/#hyperxception-class) and [HyperResNet](https://keras-team.github.io/keras-tuner/documentation/hypermodels/#hyperresnet-class) for computer vision applications.\n",
        "\n",
        "In this tutorial, you use a model builder function to define the image classification model. The model builder function returns a compiled model and uses hyperparameters you define inline to hypertune the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQKodC-jtsva"
      },
      "source": [
        "def build_model(hp):\n",
        "  '''\n",
        "  This function takes in hp, a hyperparameter class instance, and outputs a CNN model\n",
        "  '''\n",
        "  model = Sequential()\n",
        "\n",
        "  # for i in range(hp.Int('conv_blocks', 3, 5, default=3))\n",
        "  # Tune the number of units in the first Conv layer\n",
        "  # Choose an optimal value between 32-256\n",
        "  hp_units1 = hp.Int('units_conv_layer1', min_value=32, max_value=256, step=32)\n",
        "\n",
        "  # Tune the alpha value for the LeakyReLu activation in the first Conv layer\n",
        "  # Choose between 0.3 and 0.0 {0.3: default for LeakyReLu, 0.0: equivalent to ReLU)\n",
        "  hp_alpha1 = hp.Choice('alpha_conv_layer1', values = [0.3, 0.0])\n",
        "  model.add(Conv2D(filters = hp_units1, kernel_size=(3,3), padding = 'same', activation=LeakyReLU(alpha=hp_alpha1), input_shape = (28, 28, 1)))\n",
        "\n",
        "  # Tune the number of units in the second Conv layer\n",
        "  # Choose an optimal value between 32-256\n",
        "  hp_units2 = hp.Int('units_conv_layer2', min_value=32, max_value=256, step=32)\n",
        "\n",
        "  # Tune the alpha value for the LeakyReLu activation in the second Conv layer\n",
        "  # Choose between 0.3 and 0.0 {0.3: default for LeakyReLu, 0.0: equivalent to ReLU)\n",
        "  hp_alpha2 = hp.Choice('alpha_conv_layer2', values = [0.3, 0.0])\n",
        "  model.add(Conv2D(filters=hp_units2, kernel_size=(3,3), activation=LeakyReLU(alpha=hp_alpha2)))\n",
        "\n",
        "  model.add(MaxPooling2D((3,3)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Introduce dropout to surpress overfitting\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Dense(10))\n",
        "\n",
        "  # Tune the learning rate for the Adam optimizer\n",
        "  # Choose an optimal value from 0.01, or 0.001 (default of Adam).\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3])\n",
        "\n",
        "  # from_logits = True since not adding softmax yet\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),  \n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J1VYw4q3x0b"
      },
      "source": [
        "## Instantiate the tuner and perform hypertuning\n",
        "\n",
        "Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - `RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`. In this tutorial, you use the [Hyperband](https://arxiv.org/pdf/1603.06560.pdf) tuner.\n",
        "\n",
        "To instantiate the Hyperband tuner, you must specify the hypermodel, the `objective` to optimize and the maximum number of epochs to train (`max_epochs`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oichQFly6Y46"
      },
      "source": [
        "tuner = kt.Hyperband(build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='intro_to_kt')\n",
        "objective= kt.Objective(\"prec_class1\", direction=\"max\"),"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaIhhdKf9VtI"
      },
      "source": [
        "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + log<sub>`factor`</sub>(`max_epochs`) and rounding it up to the nearest integer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwhBdXx0Ekj8"
      },
      "source": [
        "Create a callback to stop training early after reaching a certain value for the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT9IkS9NEjLc"
      },
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKghEo15Tduy"
      },
      "source": [
        "Run the hyperparameter search. The arguments for the search method are the same as those used for `tf.keras.model.fit` in addition to the callback above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSBQcTHF9cKt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "93482a47-0037-487d-c448-f01e01480e4f"
      },
      "source": [
        "tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(\"The hyperparameter search is complete.\")\n",
        "\n",
        "# In the future, to better clarify the best hp, can uncomment and modify these print statements\n",
        "'''\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 30 Complete [00h 01m 56s]\n",
            "val_accuracy: 0.9073333144187927\n",
            "\n",
            "Best val_accuracy So Far: 0.9195833206176758\n",
            "Total elapsed time: 00h 27m 38s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "The hyperparameter search is complete.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(f\"\"\"\\nThe hyperparameter search is complete. The optimal number of units in the first densely-connected\\nlayer is {best_hps.get(\\'units\\')} and the optimal learning rate for the optimizer\\nis {best_hps.get(\\'learning_rate\\')}.\\n\"\"\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lak_ylf88xBv"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Find the optimal number of epochs to train the model with the hyperparameters obtained from the search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gaf4FlBfIIlC",
        "outputId": "42b0e266-2b4f-4f5a-a3ff-7bdf79b8d4d3"
      },
      "source": [
        "# Show a summary of the tuner search\n",
        "# By default, shows 10 best trials\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in my_dir/intro_to_kt\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units_conv_layer1: 160\n",
            "alpha_conv_layer1: 0.0\n",
            "units_conv_layer2: 256\n",
            "alpha_conv_layer2: 0.0\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 2\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 962c4be7e5a8ec34c2180c3ee54dbd19\n",
            "Score: 0.9195833206176758\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units_conv_layer1: 192\n",
            "alpha_conv_layer1: 0.0\n",
            "units_conv_layer2: 96\n",
            "alpha_conv_layer2: 0.0\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 2\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 754b61683c0d591877b14cab20c18281\n",
            "Score: 0.9193333387374878\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units_conv_layer1: 96\n",
            "alpha_conv_layer1: 0.0\n",
            "units_conv_layer2: 192\n",
            "alpha_conv_layer2: 0.3\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 5fd6ee9758fa141c5e0aa42832c75e0b\n",
            "Score: 0.9192500114440918\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units_conv_layer1: 224\n",
            "alpha_conv_layer1: 0.0\n",
            "units_conv_layer2: 256\n",
            "alpha_conv_layer2: 0.3\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.9170833230018616\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units_conv_layer1: 224\n",
            "alpha_conv_layer1: 0.0\n",
            "units_conv_layer2: 256\n",
            "alpha_conv_layer2: 0.3\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 35436f2ca7b59d95b4770bad0670ee1b\n",
            "Score: 0.9170833230018616\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units_conv_layer1: 96\n",
            "alpha_conv_layer1: 0.0\n",
            "units_conv_layer2: 192\n",
            "alpha_conv_layer2: 0.3\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.9165833592414856\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units_conv_layer1: 32\n",
            "alpha_conv_layer1: 0.0\n",
            "units_conv_layer2: 192\n",
            "alpha_conv_layer2: 0.3\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.9150000214576721\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units_conv_layer1: 64\n",
            "alpha_conv_layer1: 0.0\n",
            "units_conv_layer2: 192\n",
            "alpha_conv_layer2: 0.0\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.9144999980926514\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units_conv_layer1: 160\n",
            "alpha_conv_layer1: 0.0\n",
            "units_conv_layer2: 256\n",
            "alpha_conv_layer2: 0.0\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 2\n",
            "tuner/round: 1\n",
            "tuner/trial_id: dc5be97fcf48945b0710899a6951b247\n",
            "Score: 0.909333348274231\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units_conv_layer1: 224\n",
            "alpha_conv_layer1: 0.3\n",
            "units_conv_layer2: 128\n",
            "alpha_conv_layer2: 0.3\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.9073333144187927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McO82AXOuxXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e900bf36-b0c1-443d-f62b-d0e5243f4c46"
      },
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 20 epochs\n",
        "# Find the suitable number of epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(img_train, label_train, epochs=20, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.3977 - accuracy: 0.8583 - val_loss: 0.3037 - val_accuracy: 0.8892\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.2821 - accuracy: 0.8980 - val_loss: 0.2875 - val_accuracy: 0.8947\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 16s 11ms/step - loss: 0.2407 - accuracy: 0.9134 - val_loss: 0.2518 - val_accuracy: 0.9091\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.2076 - accuracy: 0.9247 - val_loss: 0.2340 - val_accuracy: 0.9153\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1854 - accuracy: 0.9324 - val_loss: 0.2318 - val_accuracy: 0.9155\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1688 - accuracy: 0.9383 - val_loss: 0.2375 - val_accuracy: 0.9147\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1483 - accuracy: 0.9455 - val_loss: 0.2367 - val_accuracy: 0.9194\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1381 - accuracy: 0.9494 - val_loss: 0.2573 - val_accuracy: 0.9168\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1306 - accuracy: 0.9520 - val_loss: 0.2343 - val_accuracy: 0.9217\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1182 - accuracy: 0.9572 - val_loss: 0.2352 - val_accuracy: 0.9206\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.1084 - accuracy: 0.9605 - val_loss: 0.2655 - val_accuracy: 0.9172\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.1020 - accuracy: 0.9624 - val_loss: 0.2468 - val_accuracy: 0.9209\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.0937 - accuracy: 0.9655 - val_loss: 0.2539 - val_accuracy: 0.9243\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.0906 - accuracy: 0.9656 - val_loss: 0.2645 - val_accuracy: 0.9172\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.0835 - accuracy: 0.9692 - val_loss: 0.2761 - val_accuracy: 0.9223\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.0795 - accuracy: 0.9697 - val_loss: 0.2782 - val_accuracy: 0.9237\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.0774 - accuracy: 0.9709 - val_loss: 0.2989 - val_accuracy: 0.9214\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.0739 - accuracy: 0.9725 - val_loss: 0.3143 - val_accuracy: 0.9198\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.0719 - accuracy: 0.9735 - val_loss: 0.3289 - val_accuracy: 0.9178\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.0683 - accuracy: 0.9740 - val_loss: 0.3253 - val_accuracy: 0.9216\n",
            "Best epoch: 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOTSirSTI3Gp"
      },
      "source": [
        "Re-instantiate the hypermodel and train it with the optimal number of epochs from above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoiPUEHmMhCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0483cd8-90c7-417a-92a7-5ec2c4b21c80"
      },
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/13\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.4005 - accuracy: 0.8588 - val_loss: 0.3009 - val_accuracy: 0.8894\n",
            "Epoch 2/13\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.2809 - accuracy: 0.8991 - val_loss: 0.2665 - val_accuracy: 0.9036\n",
            "Epoch 3/13\n",
            "1500/1500 [==============================] - 16s 11ms/step - loss: 0.2420 - accuracy: 0.9131 - val_loss: 0.2447 - val_accuracy: 0.9117\n",
            "Epoch 4/13\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.2123 - accuracy: 0.9226 - val_loss: 0.2353 - val_accuracy: 0.9140\n",
            "Epoch 5/13\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1876 - accuracy: 0.9317 - val_loss: 0.2505 - val_accuracy: 0.9130\n",
            "Epoch 6/13\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1701 - accuracy: 0.9375 - val_loss: 0.2464 - val_accuracy: 0.9113\n",
            "Epoch 7/13\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1530 - accuracy: 0.9448 - val_loss: 0.2244 - val_accuracy: 0.9208\n",
            "Epoch 8/13\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.1387 - accuracy: 0.9495 - val_loss: 0.2331 - val_accuracy: 0.9219\n",
            "Epoch 9/13\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1268 - accuracy: 0.9533 - val_loss: 0.2352 - val_accuracy: 0.9204\n",
            "Epoch 10/13\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1173 - accuracy: 0.9563 - val_loss: 0.2516 - val_accuracy: 0.9232\n",
            "Epoch 11/13\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.1078 - accuracy: 0.9602 - val_loss: 0.2402 - val_accuracy: 0.9225\n",
            "Epoch 12/13\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.1008 - accuracy: 0.9625 - val_loss: 0.2823 - val_accuracy: 0.9131\n",
            "Epoch 13/13\n",
            "1500/1500 [==============================] - 16s 10ms/step - loss: 0.0966 - accuracy: 0.9639 - val_loss: 0.2670 - val_accuracy: 0.9193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd5b80cdfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqU5ZVAaag2v"
      },
      "source": [
        "To finish this tutorial, evaluate the hypermodel on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E0BTp9Ealjb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04afd0cb-1ab1-48cb-ed3a-a20da7f5a620"
      },
      "source": [
        "eval_result = hypermodel.evaluate(img_test, label_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.2939 - accuracy: 0.9161\n",
            "[test loss, test accuracy]: [0.2939131557941437, 0.916100025177002]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq2b7CztQRUU"
      },
      "source": [
        "The test accuracy result is around 2% higher than the test accuracy reported in the original Keras Tuner guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQRpPHZsz-eC"
      },
      "source": [
        "The `my_dir/intro_to_kt` directory contains detailed logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional `overwrite=True` argument while instantiating the tuner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKwLOzKpFGAj"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, you learned how to use the Keras Tuner to tune hyperparameters for a model. To learn more about the Keras Tuner, check out these additional resources:\n",
        "\n",
        "* [Keras Tuner on the TensorFlow blog](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html)\n",
        "* [Keras Tuner website](https://keras-team.github.io/keras-tuner/)\n",
        "\n",
        "To see additional tutorials and examples of the Keras Tuner, check out:\n",
        "\n",
        "*  [Google I/O'19 Keras Tuner](https://www.youtube.com/watch?v=Un0JDL3i5Hg)\n",
        "*  [Hands on hyperparameter tuning with Keras Tuner](https://www.sicara.ai/blog/hyperparameter-tuning-keras-tuner)\n",
        "\n",
        "Also check out the [HParams Dashboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams) in TensorBoard to interactively tune your model hyperparameters."
      ]
    }
  ]
}